{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9652578,"sourceType":"datasetVersion","datasetId":5896071},{"sourceId":9677149,"sourceType":"datasetVersion","datasetId":5914543}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Introduction**\n**Project Overview :**\nThe project aims to explore natural language processing (NLP) techniques, specifically classification and text generation, using BERT (Bidirectional Encoder Representations from Transformers) on Arabic text. The tasks will involve fine-tuning the BERT model to classify articles from the KALIMAT Multipurpose Arabic Corpus and generate summaries or extended text.\n\n**Dataset :** \nThe KALIMAT Corpus, consisting of Arabic articles from six categories (culture, economy, local news, international news, religion, and sports), will be used for this project. The corpus is sourced from the Omani newspaper Alwatan.","metadata":{}},{"cell_type":"code","source":"!pip install transformers\n!pip install arabic_reshaper\n!pip install farasa\n!pip install torch\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T17:51:40.071824Z","iopub.execute_input":"2024-10-20T17:51:40.072160Z","iopub.status.idle":"2024-10-20T17:52:30.267952Z","shell.execute_reply.started":"2024-10-20T17:51:40.072121Z","shell.execute_reply":"2024-10-20T17:52:30.266739Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data Exploration**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv('/kaggle/input/twocolumns-dataset/twoColumns.csv')\nprint(df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T18:23:13.467215Z","iopub.execute_input":"2024-10-20T18:23:13.467968Z","iopub.status.idle":"2024-10-20T18:23:15.860076Z","shell.execute_reply.started":"2024-10-20T18:23:13.467927Z","shell.execute_reply":"2024-10-20T18:23:15.859024Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Preprocessing**\n\nWe begin the preprocessing by cleaning the text data, removing punctuation to make it more consistent. Next, we encode the labels into numeric values using LabelEncoder, transforming the categorical targets into a machine-readable format. Lastly, we split the dataset into training and testing sets, allowing us to train the model and evaluate its performance on unseen data.","metadata":{}},{"cell_type":"code","source":"import re\n\n# Function to clean text\ndef clean_text(text):\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    return text\n\ndf['Text'] = df['Text'].apply(clean_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T18:24:50.643045Z","iopub.execute_input":"2024-10-20T18:24:50.643724Z","iopub.status.idle":"2024-10-20T18:24:51.896085Z","shell.execute_reply.started":"2024-10-20T18:24:50.643687Z","shell.execute_reply":"2024-10-20T18:24:51.895269Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Encode the labels into numeric values\nlabel_encoder = LabelEncoder()\ndf['Label'] = label_encoder.fit_transform(df['Label'])\nprint(label_encoder.classes_)  # To see the mapping of labels to numbers\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T18:25:12.368124Z","iopub.execute_input":"2024-10-20T18:25:12.368738Z","iopub.status.idle":"2024-10-20T18:25:12.953746Z","shell.execute_reply.started":"2024-10-20T18:25:12.368702Z","shell.execute_reply":"2024-10-20T18:25:12.952817Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the dataset into training and testing sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(df['Text'], df['Label'], test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T18:26:47.155459Z","iopub.execute_input":"2024-10-20T18:26:47.156169Z","iopub.status.idle":"2024-10-20T18:26:47.165581Z","shell.execute_reply.started":"2024-10-20T18:26:47.156129Z","shell.execute_reply":"2024-10-20T18:26:47.164662Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertTokenizer\n\n# Load pre-trained BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n\n# Tokenize the text data\ntrain_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=512)\ntest_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=512)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T18:27:01.945635Z","iopub.execute_input":"2024-10-20T18:27:01.946494Z","iopub.status.idle":"2024-10-20T18:31:56.101640Z","shell.execute_reply.started":"2024-10-20T18:27:01.946433Z","shell.execute_reply":"2024-10-20T18:31:56.100598Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Model implementation**:","metadata":{}},{"cell_type":"markdown","source":"# **1- Text Classification**\n\nIn this section, we implement a BERT-based classifier to categorize Arabic text articles from the KALIMAT dataset into six distinct categories: culture, economy, local news, international news, religion, and sports. We utilize a pre-trained BERT model (bert-base-multilingual-cased) with a classification head, fine-tuning it on the dataset. The text data is tokenized using BERT's tokenizer, ensuring proper handling of Arabic script, and transformed into appropriate encodings for training. The model is then fine-tuned over three epochs, with evaluation metrics such as accuracy, precision, and recall used to measure its performance.","metadata":{}},{"cell_type":"code","source":"import torch\n\nclass ArabicDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        # Ensure both 'encodings' and 'labels' exist for the given index\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels.iloc[idx])  # Use .iloc for proper row indexing in pandas\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# Create dataset objects\ntrain_dataset = ArabicDataset(train_encodings, train_labels)\ntest_dataset = ArabicDataset(test_encodings, test_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T18:44:39.479445Z","iopub.execute_input":"2024-10-20T18:44:39.480197Z","iopub.status.idle":"2024-10-20T18:44:39.487483Z","shell.execute_reply.started":"2024-10-20T18:44:39.480151Z","shell.execute_reply":"2024-10-20T18:44:39.486452Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n\n# Load pre-trained BERT model with a classification head\nmodel = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=6)  # 6 is the number of classes\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=3,              # number of training epochs\n    per_device_train_batch_size=16,  # batch size for training\n    per_device_eval_batch_size=64,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=test_dataset            # evaluation dataset\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T18:44:46.747348Z","iopub.execute_input":"2024-10-20T18:44:46.747706Z","iopub.status.idle":"2024-10-20T18:44:47.299144Z","shell.execute_reply.started":"2024-10-20T18:44:46.747671Z","shell.execute_reply":"2024-10-20T18:44:47.298285Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(train_encodings['input_ids']), len(train_labels))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T18:44:51.076477Z","iopub.execute_input":"2024-10-20T18:44:51.076924Z","iopub.status.idle":"2024-10-20T18:44:51.082136Z","shell.execute_reply.started":"2024-10-20T18:44:51.076883Z","shell.execute_reply":"2024-10-20T18:44:51.081232Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(train_texts), len(train_labels))  # Both should be the same length\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T18:44:52.853585Z","iopub.execute_input":"2024-10-20T18:44:52.854270Z","iopub.status.idle":"2024-10-20T18:44:52.859412Z","shell.execute_reply.started":"2024-10-20T18:44:52.854226Z","shell.execute_reply":"2024-10-20T18:44:52.858437Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T18:44:55.117733Z","iopub.execute_input":"2024-10-20T18:44:55.118179Z","iopub.status.idle":"2024-10-20T18:44:55.122709Z","shell.execute_reply.started":"2024-10-20T18:44:55.118139Z","shell.execute_reply":"2024-10-20T18:44:55.121632Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T18:45:20.474162Z","iopub.execute_input":"2024-10-20T18:45:20.474551Z","iopub.status.idle":"2024-10-20T19:24:57.335961Z","shell.execute_reply.started":"2024-10-20T18:45:20.474511Z","shell.execute_reply":"2024-10-20T19:24:57.334979Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Evaluate The Classification Performance**","metadata":{}},{"cell_type":"code","source":"# Evaluate the model\ntrainer.evaluate()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T19:34:11.634133Z","iopub.execute_input":"2024-10-20T19:34:11.634508Z","iopub.status.idle":"2024-10-20T19:35:12.738356Z","shell.execute_reply.started":"2024-10-20T19:34:11.634473Z","shell.execute_reply":"2024-10-20T19:35:12.737248Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained('./arabic_bert_classifier')\ntokenizer.save_pretrained('./arabic_bert_classifier')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T19:35:34.338596Z","iopub.execute_input":"2024-10-20T19:35:34.339016Z","iopub.status.idle":"2024-10-20T19:35:36.133219Z","shell.execute_reply.started":"2024-10-20T19:35:34.338975Z","shell.execute_reply":"2024-10-20T19:35:36.132156Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the saved model and tokenizer\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport torch\n\n# Load the tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('./arabic_bert_classifier')\nmodel = BertForSequenceClassification.from_pretrained('./arabic_bert_classifier')\n\n# Example new text to classify\nnew_text = [\"الرياضة هي جزء مهم من حياة الإنسان، حيث تلعب دورًا كبيرًا في تعزيز الصحة الجسدية والعقلية. ممارسة الرياضة بشكل منتظم تساعد في تقوية العضلات وتحسين اللياقة البدنية، كما تقلل من مخاطر الإصابة بالأمراض المزمنة مثل أمراض القلب والسكري. بالإضافة إلى الفوائد الجسدية، تعزز الرياضة الثقة بالنفس وتساعد على تقليل التوتر والقلق. كما توفر الرياضة فرصة للتفاعل الاجتماعي وتعزز روح الفريق. سواء كنت تمارس رياضة فردية أو جماعية، فإن الرياضة تعد وسيلة فعالة للحفاظ على نمط حياة صحي ونشيط.\"]\n\n# Tokenize the new text\nnew_encoding = tokenizer(new_text, truncation=True, padding=True, return_tensors='pt')\n\n# Predict the label\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(**new_encoding)\n    predictions = torch.argmax(outputs.logits, dim=1)\n\n# Assuming you already have the LabelEncoder instance from preprocessing\n# Example label_encoder from training (replace with your actual label_encoder)\nfrom sklearn.preprocessing import LabelEncoder\n\n# Make sure to use the same labels as during training\ncategories = ['culture', 'economy', 'international', 'local', 'religion', 'sports']\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(categories)\n\n# Convert the predicted label from number to category name\npredicted_category = label_encoder.inverse_transform(predictions.cpu().numpy())\n\n# Print the predicted category\nprint(predicted_category[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T19:55:50.578749Z","iopub.execute_input":"2024-10-20T19:55:50.579108Z","iopub.status.idle":"2024-10-20T19:55:51.130391Z","shell.execute_reply.started":"2024-10-20T19:55:50.579076Z","shell.execute_reply":"2024-10-20T19:55:51.129136Z"},"trusted":true},"outputs":[],"execution_count":null}]}